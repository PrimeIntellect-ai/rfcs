# (Dynamic)NestedDeviceMesh

**Authors:**
* @Jackmin801
* @samsja 
* @JohannesHa

## **Summary**
The current implementation of ProcessGroups assumes a fixed rank and world size.
This results in the need to recreate ProcessGroups on all processes when there is a failure in one process.
This also means that on and off ramping processes without recreating ProcessGroups is not supported.
Currenty torch elastic implements dynamic number of nodes through restarting all the processes and loading from the last checkpoint.

However, the ProcessGroup abstraction actually wraps lazily created nccl communicators.
We could allow ProcessGroups to have dynamic world size and rank, and recreate the nccl communicators when necessary.
This would allow nodes to retry collectives that have world size independent inputs (i.e. all-reduce) when a node in the process group fails, reducing the amount of checkpointing and restarts necessary.
On-ramping compute could also be made more efficient by eliminating the needs to restart all processes.
The joining process can join the process group after warmup and RDMA the current weights from the other nodes.

## **Motivation**
- Checkpointing and restarts are a pain
- Support efficient on and off ramping

## **Proposed Implementation**

## **Drawbacks**
TBD

## **Alternatives**
- Status Quo
- Doing it at the python level

## **Prior Art**
- Our PoC for ElasticDeviceMesh

## **How we teach this**
- The change shouldn't change the way users use torch distrbuted or the torchrun launcher other than the ability for their training workload to survive gpu failures.
- Documentation should be added to inform users of the ability to on and off ramp workers in their distributed training.

## **Unresolved questions**
- **Compatibility with Functional implementation**: Does this change break some assumptions made by the Functional Comms implementation?
- **Other backends**: This RFC was written with nccl in mind. Could we and should we also implement this for other backends? (i.e. GLOO)

## **Resolution**
TBD

### Level of Support
TBD

#### Additional Context
TBD

### Next Steps
TBD

#### Tracking issue
TBD